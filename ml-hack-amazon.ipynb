{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9406705,"sourceType":"datasetVersion","datasetId":5711314}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Data Preprocessing for OCR Results","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"import os\nos.environ[\"WANDB_DISABLED\"] = \"true\"\nimport torch\nprint(torch.cuda.is_available())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\ndf = pd.read_csv('/datasets/train_data_noise120k.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\nfrom sklearn.model_selection import train_test_split\nfrom datasets import Dataset\nimport torch\n\n# Preprocess data\ndef preprocess_data(row):\n    # Combine concatenated_text and entity_name\n    input_text = f\"{row['entity_name']} ? {row['concatenated_text']}\"\n    # Target value is the entity_value\n    target_text = row['entity_value']\n    return {\"input_text\": input_text, \"target_text\": target_text}\n\n# Apply preprocessing\nprocessed_df = df.apply(preprocess_data, axis=1)\nprocessed_df = pd.DataFrame(processed_df.tolist())\n\n# Split data into train and validation\ntrain_df, val_df = train_test_split(processed_df, test_size=0.1, random_state=42)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert DataFrame to Dataset format\ntrain_dataset = Dataset.from_pandas(train_df)\nval_dataset = Dataset.from_pandas(val_df)\n\n# Load T5 model and tokenizer\nmodel_name = \"t5-small\"\ntokenizer = T5Tokenizer.from_pretrained(model_name)\nmodel = T5ForConditionalGeneration.from_pretrained(model_name)\n\n# Tokenize and encode the dataset\ndef tokenize_function(examples):\n    inputs = tokenizer(examples['input_text'], max_length=512, truncation=True, padding=\"max_length\")\n    targets = tokenizer(examples['target_text'], max_length=64, truncation=True, padding=\"max_length\")\n    inputs['labels'] = targets['input_ids']\n    return inputs\n\n# Apply tokenization\ntrain_dataset = train_dataset.map(tokenize_function, batched=True)\nval_dataset = val_dataset.map(tokenize_function, batched=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training T5 Entity Model","metadata":{}},{"cell_type":"code","source":"# Define training arguments with GPU settings\ntraining_args = TrainingArguments(\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    output_dir='./results',\n    num_train_epochs=10,\n    evaluation_strategy=\"epoch\",\n    save_total_limit=2,\n    learning_rate=2e-5,\n    weight_decay=0.01,\n    report_to=\"none\"  # Optional: Disable W&B logging\n)\n\n# Initialize Trainer\ntrainer = Trainer(\n    model=model,\n    tokenizer=tokenizer,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset\n)\n\n# Train the model\ntrainer.train()\n\n# Save the model\nmodel.save_pretrained(\"t5_entity_value_model\")\ntokenizer.save_pretrained(\"t5_entity_value_model\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Generating Predictions (Test.csv)","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\nfrom datasets import Dataset\nfrom torch.utils.data import DataLoader\nimport torch\n\n# Load the test data (without target/entity_value)\ntest_df = pd.read_csv('/datasets/test.csv')  # Adjust the path to your test.csv\n\n# Preprocess data for the test dataset\ndef preprocess_test_data(row):\n    # Combine entity_name and text for input_text\n    input_text = f\"{row['entity_name']} ? {row['text']}\"\n    return {\"input_text\": input_text}\n\n# Apply preprocessing to the test dataset\nprocessed_test_df = test_df.apply(preprocess_test_data, axis=1)\nprocessed_test_df = pd.DataFrame(processed_test_df.tolist())\n\n# Convert DataFrame to Dataset format\ntest_dataset = Dataset.from_pandas(processed_test_df)\n\n# Specify the directory where the model and tokenizer are saved\nsaved_model_dir = \"t5_entity_value_model\"\n# Load the tokenizer and model from the saved directory\ntokenizer = T5Tokenizer.from_pretrained(saved_model_dir)\nmodel = T5ForConditionalGeneration.from_pretrained(saved_model_dir)\n\n# Tokenize and encode the test dataset\ndef tokenize_test_function(examples):\n    inputs = tokenizer(examples['input_text'], max_length=512, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n    return inputs\n\n# Apply tokenization to the test dataset\ntest_dataset = test_dataset.map(tokenize_test_function, batched=True)\n\n# Convert Dataset to PyTorch DataLoader format\ndef collate_fn(batch):\n    input_ids = torch.stack([torch.tensor(item['input_ids']) for item in batch])\n    attention_mask = torch.stack([torch.tensor(item['attention_mask']) for item in batch])\n    return {'input_ids': input_ids, 'attention_mask': attention_mask}\n\nbatch_size = 32  # Adjust the batch size as needed\ntest_dataloader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=collate_fn)\n\n# Move model to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Generate predictions in batches\ntest_predictions = []\nmodel.eval()  # Set model to evaluation mode\nwith torch.no_grad():  # Disable gradient computation for faster processing\n    for batch in test_dataloader:\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n\n        # Generate predictions\n        outputs = model.generate(input_ids, attention_mask=attention_mask)\n\n        # Decode the predicted output\n        predicted_texts = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n\n        # Post-process predictions to replace \"#\" with an empty string\n        predicted_texts = [text.replace(\"#\", \"\") for text in predicted_texts]\n\n        # Append batch predictions to the final list\n        test_predictions.extend(predicted_texts)\n\n# Store the predictions in the original test DataFrame\ntest_df['prediction'] = test_predictions\n\n\n# Save the predictions to CSV\ntest_df[['index', 'prediction']].to_csv('test_predictions.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}